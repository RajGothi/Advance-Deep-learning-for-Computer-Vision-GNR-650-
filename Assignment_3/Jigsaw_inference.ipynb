{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from src.process import crop_center,get_nine_crops\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)  # Map to a 1000-dimensional vector\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool1(self.relu1(self.conv1(x)))\n",
    "        x = self.maxpool2(self.relu2(self.conv2(x)))\n",
    "        # x = self.relu3(self.conv3(x))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.relu4(self.fc1(x))\n",
    "        return x\n",
    "    \n",
    "class SelfSupervisedModel(nn.Module):\n",
    "    def __init__(self, siamese_deg=None,num_outputs=1000):\n",
    "        super(SelfSupervisedModel, self).__init__()\n",
    "        self.siamese_deg = siamese_deg\n",
    "        self.patch_model = CNNModel()\n",
    "        self.fc2 = nn.Linear(9 * 512, 4096)  # Concatenate the outputs from all patches\n",
    "        self.fc3 = nn.Linear(4096, num_outputs)  # Output layer (can serve as self.output)\n",
    "\n",
    "        # if siamese_deg == None:\n",
    "        #     self.fc2 = nn.Linear(512,num_outputs)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        # print('reach')\n",
    "        #Downstream task layer\n",
    "        if self.siamese_deg is None:\n",
    "            # print(input_batch.shape)\n",
    "            batch_features = self.patch_model(input_batch)\n",
    "            # print(\"batch: \",batch_features.shape)\n",
    "            x = self.fc2(batch_features)\n",
    "            # print(\"last x: \",x.shape)\n",
    "            x = F.log_softmax(x)\n",
    "            return x\n",
    "        # print('out')\n",
    "        #self supervised learning\n",
    "        batch_size, num_patches, channels, height, width = input_batch.size()\n",
    "\n",
    "        \n",
    "        final_feat_vectors = None\n",
    "        \n",
    "        for patch_ind in range(self.siamese_deg):\n",
    "            # Each patch_batch would be of shape (batch_size, color_channels, h_patch, w_patch)\n",
    "            patch_batch = input_batch[:, patch_ind, :, :, :]\n",
    "            patch_batch_features = self.patch_model(patch_batch)\n",
    "\n",
    "            if patch_ind == 0:\n",
    "                final_feat_vectors = patch_batch_features\n",
    "            else:\n",
    "                final_feat_vectors = torch.cat([final_feat_vectors, patch_batch_features], dim=1)\n",
    "        \n",
    "        # Use fc3 as the output layer\n",
    "        x = self.fc2(final_feat_vectors)\n",
    "        x = F.log_softmax(self.fc3(x))\n",
    "        # x = F.log_softmax(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class GetJigsawPuzzleDataset(Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, file_paths, avail_permuts_file_path, range_permut_indices=None, transform=None):\n",
    "        'Initialization'\n",
    "        self.file_paths = file_paths\n",
    "        self.transform = transform\n",
    "        self.permuts_avail = np.load(avail_permuts_file_path)\n",
    "        self.range_permut_indices = range_permut_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "\n",
    "        # Select sample\n",
    "        file_path = self.file_paths[index]\n",
    "        pil_image = Image.open(file_path)\n",
    "        orig_image = pil_image\n",
    "        # Check if image has only single channel. If True, then swap with 0th image\n",
    "        # Assumption 0th image has got 3 number of channels\n",
    "        if len(pil_image.getbands()) != 3:\n",
    "            file_path = self.file_paths[0]\n",
    "            pil_image = Image.open(file_path)\n",
    "\n",
    "        # Convert image to torch tensor\n",
    "        pil_image = pil_image.resize((128, 128))\n",
    "        pil_image = crop_center(pil_image, 105, 105)\n",
    "\n",
    "        # Get nine crops for the image\n",
    "        nine_crops = get_nine_crops(pil_image)\n",
    "\n",
    "        # Permut the 9 patches obtained from the image\n",
    "        if self.range_permut_indices:\n",
    "            permut_ind = random.randint(self.range_permut_indices[0], self.range_permut_indices[1])\n",
    "        else:\n",
    "            permut_ind = random.randint(0, len(self.permuts_avail) - 1)\n",
    "\n",
    "        permutation_config = self.permuts_avail[permut_ind]\n",
    "\n",
    "        permuted_patches_arr = [None] * 9\n",
    "        for crop_new_pos, crop in zip(permutation_config, nine_crops):\n",
    "            permuted_patches_arr[crop_new_pos] = crop\n",
    "\n",
    "        # Apply data transforms\n",
    "        tensor_patches = torch.zeros(9, 3, 32, 32)\n",
    "        for ind, jigsaw_patch in enumerate(permuted_patches_arr):\n",
    "            jigsaw_patch_tr = self.transform(jigsaw_patch)\n",
    "            tensor_patches[ind] = jigsaw_patch_tr\n",
    "\n",
    "        return tensor_patches, permut_ind\n",
    "    \n",
    "def get_paths():\n",
    "    data_dir = 'CIFAR-10-images/test'\n",
    "    file_paths_to_return = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\"):\n",
    "                file_paths_to_return.append(root+'/'+file)                        \n",
    "    \n",
    "    return file_paths_to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jigsaw inference to check model performance on Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['module.patch_model.conv1.weight', 'module.patch_model.conv1.bias', 'module.patch_model.conv2.weight', 'module.patch_model.conv2.bias', 'module.patch_model.conv3.weight', 'module.patch_model.conv3.bias', 'module.patch_model.fc1.weight', 'module.patch_model.fc1.bias', 'module.fc2.weight', 'module.fc2.bias', 'module.fc3.weight', 'module.fc3.bias'])\n",
      "Model loaded successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SelfSupervisedModel(\n",
       "  (patch_model): CNNModel(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU()\n",
       "    (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu2): ReLU()\n",
       "    (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu3): ReLU()\n",
       "    (fc1): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    (relu4): ReLU()\n",
       "  )\n",
       "  (fc2): Linear(in_features=4608, out_features=4096, bias=True)\n",
       "  (fc3): Linear(in_features=4096, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for downstream classification\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset,random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms, utils, models\n",
    "from src.trainer import ModelTrainTest\n",
    "from torchvision import transforms\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.RandomCrop((32, 32)),\n",
    "    transforms.ColorJitter(brightness=[0.5, 1.5]),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Set device to use to gpu if available and declare model_file_path\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "test_file_path = get_paths()\n",
    "permuts_file_path = 'Data/selected_permuts.npy'\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    GetJigsawPuzzleDataset(test_file_path, permuts_file_path, transform=data_transform),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=8\n",
    ")\n",
    "\n",
    "# Train required model defined above on CUB200 data\n",
    "num_classes = 10\n",
    "\n",
    "model_to_test = SelfSupervisedModel(siamese_deg=9)\n",
    "\n",
    "checkpoint_path = 'Model/jigsaw_solver_CIFAR-10_trained.pt'\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "print(checkpoint.keys())\n",
    "\n",
    "new_checkpoint = {}\n",
    "\n",
    "# Iterate through the keys in the loaded checkpoint\n",
    "for key, value in checkpoint.items():\n",
    "    # Modify the key to match the existing model's module name\n",
    "    new_key = key.replace('module.', '')  # Remove 'module.' prefix if it exists\n",
    "    new_checkpoint[new_key] = value\n",
    "\n",
    "model_to_test.load_state_dict(new_checkpoint)\n",
    "\n",
    "print('Model loaded successfully')\n",
    "\n",
    "# Set device on which training is done. Plus optimizer to use.    \n",
    "model_to_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_correct_preds(network_output, target):\n",
    "\n",
    "    output = network_output\n",
    "    pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "    pred.data = pred.data.view_as(target.data)\n",
    "    correct = target.eq(pred).sum().item()\n",
    "\n",
    "    return correct\n",
    "\n",
    "def test(network,test_data_loader):\n",
    "        network.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(test_data_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n",
    "\n",
    "            correct += count_correct_preds(output, target)\n",
    "\n",
    "            del data, target, output\n",
    "\n",
    "        test_loss /= len(test_data_loader.dataset)\n",
    "        test_acc = correct / len(test_data_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_data_loader.dataset),\n",
    "            100. * correct / len(test_data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1319647/1412787335.py:81: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(self.fc3(x))\n",
      "/home/raj/.conda/envs/raj/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1855, Accuracy: 9533/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(network=model_to_test,test_data_loader = test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jigsaw Downstream Task(Image classification) inference on Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "normal_data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['module.patch_model.conv1.weight', 'module.patch_model.conv1.bias', 'module.patch_model.conv2.weight', 'module.patch_model.conv2.bias', 'module.patch_model.conv3.weight', 'module.patch_model.conv3.bias', 'module.patch_model.fc1.weight', 'module.patch_model.fc1.bias', 'module.fc2.weight', 'module.fc2.bias', 'module.fc3.weight', 'module.fc3.bias'])\n",
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "#for downstream classification\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, ConcatDataset,random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms, utils, models\n",
    "from src.trainer import ModelTrainTest\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Set device to use to gpu if available and declare model_file_path\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "test_dataset = ImageFolder(root='CIFAR-10-images/test',transform=normal_data_transform)\n",
    "\n",
    "test_loader =  torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size,shuffle=False)\n",
    "\n",
    "# Train required model defined above on CUB200 data\n",
    "num_classes = 10\n",
    "\n",
    "model_to_test = SelfSupervisedModel(siamese_deg=None)\n",
    "model_to_test.fc2 = nn.Linear(512,num_classes)\n",
    "checkpoint_path = 'Model/jigsaw_downstream_image_recognition.pt'\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "print(checkpoint.keys())\n",
    "\n",
    "new_checkpoint = {}\n",
    "\n",
    "# Iterate through the keys in the loaded checkpoint\n",
    "for key, value in checkpoint.items():\n",
    "    # Modify the key to match the existing model's module name\n",
    "    new_key = key.replace('module.', '')  # Remove 'module.' prefix if it exists\n",
    "    new_checkpoint[new_key] = value\n",
    "\n",
    "model_to_test.load_state_dict(new_checkpoint)\n",
    "\n",
    "# print(model_to_train)\n",
    "# Load state dict for pre trained model weights\n",
    "model_to_test = model_to_test.to(device)\n",
    "print('Model loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classification(network,test_data_loader):\n",
    "        network.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(test_data_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n",
    "\n",
    "            correct += count_correct_preds(output, target)\n",
    "\n",
    "            del data, target, output\n",
    "\n",
    "        test_loss /= len(test_data_loader.dataset)\n",
    "        test_acc = correct / len(test_data_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_data_loader.dataset),\n",
    "            100. * correct / len(test_data_loader.dataset)))\n",
    "\n",
    "        return  test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1319647/1412787335.py:60: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.0222, Accuracy: 6662/10000 (67%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0222248842716217, 0.6662)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_classification(model_to_test,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
